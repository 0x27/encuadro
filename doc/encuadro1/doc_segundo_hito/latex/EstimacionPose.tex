% !TEX encoding = IsoLatin
\chapter{Estimación de pose monocular}

%Definición de realidad aumentada, ejemplos. Breve mención de las cosas que se usan, modelo cámara pinhole, calibración, extracción de características(LSD,ORT), marcador utilizado.
%Algoritmos de estimación de pose. Algoritmos de correspondencias. Toda la rama Posit
%Habria que hacer un diagrama de bloques que muestre el flujo para hallar la pose

Se le llama ``estimación de pose'' al proceso mediente el cual se calcula en qué punto del mundo y con qué orientación se encuentra determinado objeto respecto de un eje de coordenadas previamente definido al que se lo llama \textit{ejes del mundo}. Las aplicaciones de realidad aumentada requieren de un modelado preciso del entorno respecto de estos ejes, para poder ubicar correctamente los agregados virtuales dentro del modelo y luego renderizarlos en la imagen vista por el usuario. El objeto cuya estimación de pose resulta de mayor importancia es la cámara, ya que por ésta es por donde se mira la escena y es respecto de ésta que los objetos virtuales deben ubicarse de manera consistente.  Una forma de estimar la pose de la cámara es mediante el uso de las imágenes capturadas por ella misma.\\ 
Asimismo, el concepto ``monocular'' hace referencia al uso de una sola cámara, ya que es posible trabajar con más de una.\\
Para poder estimar la pose de una cámara, resulta necesario modelarla adecuadamente ya que no todas las cámaras son iguales. El modelo más comunmente utilizado es el denominado \textit{pin-hole}. Para modelar completamente la cámara se deben estimar ciertos \textit{parámetros intrínsecos} a ésta, y eso se logra luego de realizados algunos experimentos. A la estimación de estos parámetros se le denomina \textit{calibración de cámara}.
%\begin{figure}[h]
%\centering
%\includegraphics[scale=1.5]{../imagenes/EstPose/EstPose_1.eps}
%\caption{Diagrama de bloques para el proceso de estimación de pose.}
%\label{EstPose_1}
%\end{figure}
%En este capítulo se presentan las técnicas realizadas para implementar la realidad aumentada


\section{Calibración de cámara: modelo pin-hole \cite{garciaocon07}}
\label{sec:Calibracion de camara}

% ------------------------------------------------------ FUNDAMENTOS Y DEFINICIONES -------------------------------------------------------
\subsection{Fundamentos y definiciones}
\label{sec:Fundamentos y definiciones}
Este modelo consiste en un centro óptico C, en donde convergen todos los rayos de la proyección y un plano imagen en el cual la imagen es proyectada. Se define ``distancia focal'' ($f$) como la distancia entre el centro óptico C y el cruce del eje óptico por el plano imagen (punto P). Ver imagen \ref{fig:CalibracionCamara}.  

\begin{figure}[h!]
\centering
\includegraphics[scale=1.0]{imagenes/EstPose/CalibracionCamara.eps}
\caption{Modelo de cámara pin-hole.}
\label{fig:CalibracionCamara}
\end{figure}

Para modelar el proceso de proyección (proceso en el que se asocia al punto \textbf{M} del mundo, un punto \textbf{m} en la imagen), es necesario referirse a varias transformaciones y varios ejes de coordenadas.
\begin{itemize}
\item \textit{Coordenadas del mundo}: son las coordenadas que describen la posición 3D del punto \textbf{M}. Se definen respecto de los \textit{ejes del mundo} $(X_m,Y_m,Z_m)$. La elección de los ejes del mundo es arbitraria.
\item \textit{Coordenadas de la cámara}: son las coordenadas que describen la posición del punto \textbf{M} respecto de los ejes de la cámara $(X,Y,Z)$.
\item \textit{Coordenadas de la imagen}: son las coordenadas que describen la posición del punto 2D, \textbf{m}, respecto del centro del plano imagen, P. Los ejes de este sistema de coordenadas son $(u,v)$.
\item \textit{Coordenadas normalizadas de la imagen}: son las coordenadas que describen la posición del punto 2D, \textbf{m}, respecto del eje de coordenadas $(u',v')$ situado en la esquina superior izquierda del plano imagen.
\end{itemize}
La transformación que lleva del punto \textbf{M}, expresado respecto de las coordenadas del mundo, al punto \textbf{m}, expresado respecto del sistema de coordenadas normalizadas de la imagen, se puede ver como la composición de dos transformaciones menores. La primera, es la que realiza la proyección que transforma a un punto definido respecto del sistema de coordenadas de la cámara $(X,Y,Z)$ en otro punto sobre el plano imagen expresado respecto del sistema de coordenadas normalizadas de la imagen $(u',v')$. Véase que una vez calculada esta transformación, es una constante característica de cada cámara. Se le llama al conjunto de valores que definen esta transformación \textit{parámetros intrínsecos} de la cámara. La segunda, es la transformación que lleva de expresar a un punto respecto de los ejes del mundo $(X_m,Y_m,Z_m)$, a los ejes de la cámara $(X,Y,Z)$. Esta última transformación varía conforme se mueve la cámara (respecto de los ejes del mundo) y el conjunto de valores que la definen es denominado \textit{parámetros extrínsecos} de la cámara. Del cálculo de estos parámetros es que se obtiene la estimación de la pose de la cámara.\\
De lo anterior se concluye rápidamente que si se le llama $PY$ a la matriz proyección total, tal que:
\[
m = PY.M,
\]
entonces:
\[
PY = I.E
\]
donde $I$ corresponde a la matriz proyección asociada a los parámetros intrínsecos y $E$ corresponde a la matriz asociada a los parámetros extrínsecos.\\

\begin{itemize}
\item \textbf{Parámetros extrínsecos:} pose de la cámara.
\begin{itemize}
\item \underline{Traslación:} ubicación del centro óptico de la cámara respecto de los ejes del mundo.
\item \underline{Rotación:} rotación del sistema de coordenadas de la cámara $(X,Y,Z)$, respecto de los ejes del mundo.
\end{itemize}
\item \textbf{Parámetros intrínsecos:} parámetros propios de la cámara. Dependen de su geometría interna y de su óptica.
\begin{itemize}
\item \underline{Punto principal (P = [$u'_P,v'_P$]):} es el punto intersección entre el eje óptico y el plano imagen. Las coordenadas de este punto vienen dadas en píxeles y son expresadas respecto del sistema normalizado de la imagen.
\item \underline{Factores de conversión píxel-milímetros ($d_u,d_v$):} indican el número de píxeles por milímetro que usa la cámara en las direcciones $u$ y $v$ respectivamente.
\item \underline{Distancia focal (f):} distancia entre el centro óptico (\textbf{C}) y el punto principa (\textbf{P}). Su unidad es el milímetro.
\item \underline{Factor de proporción (s):} indica la proporción entre las dimensiones horizontal y vertical de un píxel.   
\end{itemize}
\end{itemize}

% ------------------------------------------------------ MATRIZ DE PROYECCION -------------------------------------------------------
\subsection{Matriz de proyección}
\label{sec:Matriz de proyección}

En la sección anterior se vió que es posible hallar una ``matriz de proyección'' PY que dependa tanto de los parámetros intrínsecos de la cámara como se sus parámetros extrínsecos:
\[
m = PY.M
\] 
donde \textbf{M} y \textbf{m} son los puntos ya definidos y vienen expresados en \textit{coordenadas homogéneas}. Por más información acerca de este tipo de coordenadas ver \cite{Hartley2004}.\\
Para determinar la forma de la matriz de proyección se estudia cómo se relacionan las coordenadas de \textbf{M} con las coordenads de \textbf{m}; para hallar esta relación se debe analizar cada transformación, entre los sistemas de coordenadas mencionados con anterioridad, por separado.\\
\begin{itemize}
\item \textbf{Proyección 3D - 2D:} de las coordenadas homogéneas del punto \textbf{M} expresadas en el sistema de coordenadas de la cámara $(X_0,Y_0,Z_0,T_0)$, a las coordenadas homogéneas del punto \textbf{m} expresadas en el sistema de coordenadas de la imagen $(u_0,v_0,s_0)$:\\
Se desprende de la imagen \ref{fig:CalibracionCamara} y algo de geometría proyectiva la siguiente relación entre las coordenadas y la distancia focal (f):
\[
\frac{f}{Z_0} = \frac{u_0}{X_0} = \frac{v_0}{Y_0}
\]
A partir de la relación anterior:
\[
\left( \begin{array}{c}
u_0 \\
v_0
\end{array} \right)
=
\frac{f}{Z_0} 
\left( \begin{array}{c}
X_0 \\ 
Y_0
\end{array} \right)
\]
Expresado en forma matricial, en coordenadas homogéneas:
\[
\left( \begin{array}{c}
u_0 \\
v_0 \\
s_0
\end{array} \right)
= 
\left( \begin{array}{cccc}
f & 0 & 0 & 0 \\ 
0 & f & 0 & 0 \\
0 & 0 & 1 & 0
\end{array} \right)
\left( \begin{array}{c}
X_0 \\ 
Y_0 \\
Z_0 \\
1
\end{array} \right)
\]
\item \textbf{Transformación cámara imagen:} de las coordenadas homogéneas del punto \textbf{m} expresadas respecto del sistema de coordenadas de la imagen $(u_0,v_0,s_0)$, a las coordenadas homogéneas de él mismo pero expresadas respecto del sistema de coordenadas normalizadas de la imagen $(u'_0,v'_0,s'_0)$:\\

\begin{figure}[h!]
\centering
\includegraphics[scale=1.0]{imagenes/EstPose/ParametrosIntrinsecos.eps}
\caption{Relación entre el sistema de coordenadas de la imagen y el sistema de coordenandas normalizadas de la imagen.}
\label{fig:ParametrosIntrinsecos}
\end{figure}


Se les suma, a las coordenadas de \textbf{m} respecto del sistema de la imagen, la posición del punto P respecto del sistema normalizado de la imgen $(u'_P,v'_P)$. Las coordenadas de \textbf{m} dejan de expresarse en milímetros para expresarse en píxeles. Aparecen los factores de conversión $d_u$ y $d_v$:
\[
\begin{array}{c}
u'_0 = d_u.u_0 + u'_P \\
v'_0 = d_v.v_0 + v'_P
\end{array}
\]
Se obtiene entonces la siguiente relación matricial, en coordenadas homogéneas:
\[
\left( \begin{array}{c}
u'_0 \\
v'_0 \\
s'_0
\end{array} \right)
= 
\left( \begin{array}{ccc}
d_u & 0 & u'_P \\ 
0 & d_v & v'_P \\
0 & 0 & 1
\end{array} \right)
\left( \begin{array}{c}
u_0 \\ 
v_0 \\
1
\end{array} \right)
\]
\item \textbf{Matriz de parámetros intrínsecos $(I)$:} de las coordenadas homogéneas del punto \textbf{M} expresadas en el sistema de coordenadas de la cámara $(X_0,Y_0,Z_0,T_0)$, a las coordenadas homogéneas del punto \textbf{m} expresadas respecto del sistema de coordenadas normalizadas de la imagen $(u'_0,v'_0,s'_0)$:\\
Se obtiene combinando las dos últimas transformaciones. Nótese que como ya se aclaró, depende únicamente de parámetros propios de la construcción de la cámara:
\[
I = 
\left( \begin{array}{cccc}
d_u.f & 0 & u'_P & 0\\ 
0 & d_v.f & v'_P & 0\\
0 & 0 & 1 & 0
\end{array} \right)
\]
\item \textbf{Matriz de parámetros extrínsecos $(E)$:}  de las coordenadas homogéneas del punto \textbf{M} expresadas respecto del sistema de coordenadas del mundo $(X_{m0}, Y_{m0}, Z_{m0}, T_{m0})$, a las coordenadas homogéneas de él mismo pero expresadas respecto del sistema de coordenadas de la cámara $(X_0,Y_0,Z_0,T_0)$:\\
Se obtiene de estimar la pose de la cámara respecto de los ejes del mundo y es la combinación de, primero una rotación $R$, y luego una traslación $T$. Se obtiene entonces la siguiente representación matricial:\\
\[
\left( \begin{array}{c}
X_0 \\
Y_0 \\
Z_0 \\
T_0
\end{array} \right)
= 
\left( \begin{array}{cc}
R & T \\ 
0 & 1
\end{array} \right)
\left( \begin{array}{c}
X_{m0} \\ 
Y_{m0} \\
Z_{m0} \\
T_{m0}
\end{array} \right)
\]
donde la matriz de parámetros extrínsecos desarrollada toma la forma:
\[
E =
\left( \begin{array}{cccc}
r_{11} & r_{12} & r_{13} & t_{x} \\ 
r_{21} & r_{22} & r_{23} & t_{y}\\
r_{31} & r_{32} & r_{33} & t_{z} \\
0 & 0 & 0 & 1
\end{array} \right)
\]  
\item \textbf{Matriz de proyección $(PY)$:}  de las coordenadas homogéneas del punto \textbf{M} expresadas respecto del sistema de coordenadas del mundo $(X_{m0}, Y_{m0}, Z_{m0}, T_{m0})$, a las coordenadas homogéneas del punto \textbf{m} expresadas respecto del sistema de coordenadas normalizadas de la imagen $(u'_0,v'_0,s'_0)$:\\
Es la proyección total y se obtiene combinando las dos transformaciones anteriores:
\[
\left( \begin{array}{c}
u'_0 \\
v'_0 \\
s'_0
\end{array} \right)
= 
\left( \begin{array}{cccc}
d_u.f & 0 & u'_P & 0\\ 
0 & d_v.f & v'_P & 0\\
0 & 0 & 1 & 0
\end{array} \right)
.
\left( \begin{array}{cccc}
r_{11} & r_{12} & r_{13} & t_{x} \\ 
r_{21} & r_{22} & r_{23} & t_{y}\\
r_{31} & r_{32} & r_{33} & t_{z} \\
0 & 0 & 0 & 1
\end{array} \right)
.
\left( \begin{array}{c}
X_{m0} \\ 
Y_{m0} \\
Z_{m0} \\
T_{m0}
\end{array} \right)
\]
\underline{Nota:} Este modelo no tiene en cuenta los efectos de distorsión de la lente.
\end{itemize}
% -----------------------------------------------------------------------------------------------------------------
% ----------------------------------------------------------------------------------------------------------------
\section{Algoritmos de extracción de características}
\label{sec:Features}

\subsection{Segmentos}

\section{Marcadores}
\label{sec:Marcador}
La inclusión de \emph{marcadores}, \emph{marcas de referencia} o \emph{fiduciales}, en inglés \emph{markers}, \emph{landmarks} o \emph{fiducials}, en la escena ayuda al problema de extracción de características y por lo tanto al problema de estimación de pose \cite{Lepetit05b}. Estos por construcción son elementos que presentan una detección estable en la imagen para el tipo de característica que se desea extraer así como medidas facilmente utilizables para la estimación de la pose.

Se distinguen dos tipos de \emph{fiduciales}. El primer tipo son los que se llaman puntos \emph{fiduciales} por que proveen una correspondencia de puntos entre la escena y la imagen. El segundo tipo, \emph{fiduciales planares}, se pueden obtener mediante la construcción en una geometría coplanar de una serie de \emph{puntos fiduciales} identificables como esquinas. Un único \emph{fiducial planar} puede contener por si solo todas las seis restricciones espaciales necesarias para definir el marco de coordenadas. 

Como se explica en la sección \ref{sec:EstimacionPose} el problema de estimación de pose requiere de una serie de correspondencias $\mathbf{M}_i\leftrightarrow \mathbf{m}_i$ entre el puntos 3D en la escena en coordenadas del mundo y puntos en la imagen. El enfoque elegido 

\subsection{Marcador QR}
El enfoque inicial elegido para la detección de \emph{puntos fiduciales} para marcadores parte del trabajo de fin de curso de Matías Tailanian para el curso \emph{Tratamiento de imágenes por computadora} de Facultad de Ingeniería, Universidad de la Republica\footnote{Autoposicionamiento 3D -  http://sites.google.com/site/autoposicionamiento3d/}. La elección se basa principalmente en los buenos resultados obtenidos para dicho trabajo con un enfoque relativamente simple. El trabajo desarrolla, entre otras cosas, un diseño de marcador y un sistema de detección de marcadores basado en el detector de segmentos LSD\cite{grompone10} por su buena performance y aparente bajo costo computacional. 

El marcador utilizado está basado en la estructura de detección incluida en los códigos \emph{QR} y se muestra en la figura \ref{fig:Marker}. Éste consiste en trés grupos idénticos de tres cuadrados concéntricos superpuestos de tal forma que los lados de cada uno de trés cuadrados son visualizables. A diferencia de los códigos \emph{QR} la disposición de los grupos de cuadrados se distintos para evitar ambiguedades en la determinación  de su posicionamiento espacial. Estas dos características son escenciales para la extracción de los \emph{puntos fiduciales} de forma coherente, es decir, las correspondencias tienen que poder ser determinadas completamente bajo criterios razonables.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.3]{imagenes/marker/Marker.eps}
\caption{Marcador propuesto basado en la estructura de detección de códigos QR.}
\label{fig:Marker}
\end{figure}

\subsubsection{Elementos estructura del marcador}
En la presente subsección se presentan algunas definiciones de las estructuras formantes del marcador. Estas seran de útilidad para el diseño y formaran un flujo natural y escalable para el desarrollo del algoritmo de determinación de correspondencias.

Los elementos mas básicos en la estructura son los \emph{segmentos} los cuales consisten en un par de puntos en la imagen, $\mathbf{p} = (p_x,p_y)$ y $\mathbf{q} = (q_x,q_y)$. Estos segmentos serán los lados del cuadrilátero, el próximo elemento en la estructura del marcador.

Un \emph{cuadrilátero} o \texttt{Ql} está determinado por cuatro segmentos conexos y distintos entre sí. La propiedad de conexión se relaja a la intersección de dos discos de un cierto radio en torno a los puntos de dos segmentos vecinos. El cuadrilátero tiene dos propiedades notables; el \emph{centro} definido como el punto medio de sus vértices y el \emph{perímetro} definido como la suma de sus cuatro lados. Los \emph{vertices} de un cuadrilátero se determinan mediante la intersección, en un sentido amplio, de dos segmentos contiguos.

Un \emph{grupo de cuadriláteros} o \texttt{QlSet}, se construye a partir de \texttt{M} cuadriláteros, de distinto tamaño, que comparten un mismo centro, con $\texttt{M}>1$. A partir de dichos cuadriláteros se construye un lista ordenada $(\texttt{Ql[0]},\texttt{Ql[1]},\dots,\texttt{Ql[M-1]})$ en donde el orden viene dado por el valor de perímetro de cada \texttt{Ql}. Se define el \emph{centro del grupo de cuadriláteros} como el baricentro, o promedio ponderado, de los centros de cada \texttt{Ql} de la lista ordenada.

Finalmente el \emph{marcador QR} estará constituido por \texttt{N} \emph{grupos de cuadriláteros} dispuestos en una geometría particular. Esta geometría debe determinar un sistema de coordenadas, un origen y dos ejes. Se tendrá una lista ordenada  $(\texttt{QlSet[0]},\texttt{QlSet[1]},\dots,\texttt{QlSet[N-1]})$ en donde el orden se determinará mediante la geometría de los mismos.

Un marcador proveerá un numero de $\texttt{4}\times\texttt{M}\times\texttt{N}$ vertices y por lo tanto la misma cantidad de puntos fiduciales para proveer las correspondencias  $\mathbf{M}_i\leftrightarrow \mathbf{m}_i$ para el algoritmo de estimación de pose.

\subsubsection{Diseño}
Un detalle del marcador se muestra en la figura \ref{fig:MarkerDetail} en donde se define el grupo \texttt{i} de cuadrados concentricos como el \texttt{QlSet[i]} (sets de cuadriláteros) y se definen los respectivos centros $\mathbf{c}_i$ para cada \texttt{QlSet[i]}. Se considera además un eje de coordenadas que queda definido los vectores normalizados.
$$\mathbf{x} = \frac{\mathbf{c}_1 - \mathbf{c}_0}{||\mathbf{c}_1-\mathbf{c}_0||}$$
$$\mathbf{y} = \frac{\mathbf{c}_2 - \mathbf{c}_0}{||\mathbf{c}_2-\mathbf{c}_0||}$$
Por otro lado la disposición de los \texttt{QlSet} es tal que la distancia indicada $\mathbf{d}_{01}$ definida como la norma del vector entre los centros $\mathbf{c}_1$ y $\mathbf{c}_0$ es significativamente mayor que la distancia $\mathbf{d}_{02}$ definida como la norma del vector entre los centros $\mathbf{c}_2$ y $\mathbf{c}_1$.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.3]{imagenes/marker/MarkerDetail.eps}
\caption{Detalle del marcador propuesto formando un sistema de coordenadas.}
\label{fig:MarkerDetail}
\end{figure}

En base al sistema de coordenadas definido en la figura \ref{fig:MarkerDetail} se puede fijar un orden determinado para los \emph{puntos fiduciales} que serán utilizados como correspondencias entre la imagen y la escena. Éstos se toman partiendo del cuadrado 
\begin{figure}[ht]
\centering
\includegraphics[scale=0.4]{imagenes/marker/QlSetDetail.eps}
\caption{Detalle de un QlSet indicando el orden de los puntos basados en el eje de coordenadas definido previamente.}
\label{fig:QlSetDetail}
\end{figure}

\subsubsection{Filtrado de segmentos}


\subsubsection{Determinación de correspondencias}


\section{Algoritmos de estimación de pose monocular}
\label{sec:EstimacionPose}

La estimación de pose consiste en calcular la posición y la orientación de un objeto a partir de una imagen del mismo. Dentro las técnicas de estimación de pose, se encuentran aquellas en las que se debe conocer la correspondencia entre puntos de la imagen y puntos del modelo, y aquella en las que la correspondencia se resuelve en conjunto con la estimacion de pose.\@

 

conociendo la correspondencia entre puntos caractoreisticos en la imagen y puntos del objeto. . Mediante los algoritmos de extracción de características vistos anteriormente se obtienen puntos claves del objeto los 

\begin{itemize}
\item POSIT
\item BABAB
\item ABABA
\item BABAB
\end{itemize}

\section{POSIT}
\label{sec:Posit}
El algortimo Posit permite calcular la pose de un objeto conocido a partir de una imagen en la que se conocen puntos caracteristicos del objeto. Las correspondencias entre los puntos dectados en la imagen y los puntos del objeto deben ser conocidas y deben ser mas de cuatro. En la version orignal del algoritmo, presentado en(ref), los puntos del objeto utilizados para estimar la pose no deben ser coplanares. Debido a que el marcador utilizado es plano fue necesario utilizar una version del algoritmo que soporta puntos del modelo coplanares(ref).  

\subsection{Hipotesis de trabajo}

\begin{itemize}
\item Modelo de cámara  pinhole. El centro de la cámara esta en el punto $O$, el plano de imagen $G$ se encuentra a distancia $f$ de $O$. Los ejes $O_x$ y $O_y$ apuntan según las filas y columnas del sensor de la cámara, el eje $O_z$ apunta según el eje óptico. Los versores para estos ejes son $i$, $j$ y $k$.
\item $n$ puntos $M_0$, $M_1$,...$M_i$,...$M_{n-1}$ del objeto, ubicados en el campo de vista de la cámara($fov$).
\item Se toma $M_0$ como punto de referencia del objeto, el eje de coordenadas respecto al objeto queda definido por ($u$,$v$,$w$). Como la geometría del objeto se asume conocida, también se conocen las coordenadas ($U_i$,$V_i$,$W_i$) de los puntos $M_i$ en el eje de coordenadas del objeto
\item  Las imágenes de los puntos $M_i$ son los puntos $m_i$ con coordenadas ($x_i$,$y_i$) en el plano imagen también conocidas.
\item  Las coordendas ($X_i$,$Y_i$,$Z_i$) de los puntos $M_i$ en el eje de coordenadas de la cámara son desconocidas. 
\end{itemize}

%ESTAS COSAS (\input) A MI NO ME COMPILAN
% PABLO 

%\begin{figure}[h]
%\centering
%\input{../imagenes/EstPose/POSIT/EstPose_Posit_1.pdf_tex}
%\caption{Proyeccion perspectiva ($m_i$) y $SOP$ ($p_i$) para un punto del objeto $M_i$ y un punto de referencia $M_0$.}
%\label{fig:EstPose_Posit_1}
%\end{figure}

\subsection{Problema a resolver}
Se busca computar la matriz de rotación y el vector de traslación del objeto. La matriz de rotación \textbf{R} del objeto, es la matriz cuyas filas son las coordenadas del los versores $i$, $j$ y $k$ del sistema de coordenadas de la cámara en el sistema de coordenadas del objeto ($u$,$v$,$w$).

La matriz  \textbf{R}  queda:


\[ R=\left( \begin{array}{ccc}
i_u & i_v & i_w \\
j_u & j_v & j_w \\
k_u & k_v & k_w \end{array} \right)\] \\

Para obtener la matriz de rotación solo es necesario obtener los versores  \textbf{i}  y  \textbf{j} , el versor  \textbf{k}  se obtiene de realizar el producto vectorial  \textbf{i} $\times$  \textbf{j}. El vector de traslación es el vector que va del centro del objeto $M_0$ a el centro del sistema de coordenadas de la cámara $O$. Por lo tanto las coordenadas del vector de traslación son ($X_0$,$Y_0$,$Z_0$). Si este punto $M_0$ es uno de los puntos visibles en la imagen, entonces el vector \textbf{T}  esta alineado con el vector $Om_0$ y es igual a ($Z_0$/$f$)$Om_0$.\\
Por lo tanto la pose queda determinada si se conocen  \textbf{i}, \textbf{j} y  \textbf{$Z_0$}. 

%En la figura \ref{EstPose_Posit_1} se muestra el modelo de cámara pinhole. El centro de la cámara esta en el punto $O$, el plano de imagen $G$ se encuentra a distancia $f$ de $O$. Los ejes $O_x$ y $O_y$ apuntan según las filas y columnas del sensor de la cámara, el eje $O_z$ apunta según el eje óptico. Los versores para estos ejes son $i$, $j$ y $k$.\\
%Supongamos un objeto con $n$ puntos $M_0$, $M_1$,...$M_i$,...$M_n-1$, ubicados en el campo de vista de la cámara($fov$). Si se toma $M_0$ como punto de referencia del objeto, entonces el eje de coordenadas respecto al objeto queda definido por ($u$,$v$,$w$). Como la geometría del objeto se asume conocida, también se conocen las coordenadas ($U_i$,$V_i$,$W_i$) de los puntos $M_i$ en el eje de coordenadas del objeto. Las imágenes de los puntos $M_i$ son los puntos $m_i$ con coordenadas ($x_i$,$y_i$) en el plano imagen %


\subsection{Proyeccion ortografica escalada (SOP)}
La SOP es una aproximacion a la proyeccion perspecitva. Dado cualquier punto del objeto $M_i$ con coordenadas  ($X_i$,$Y_i$,$Z_i$), se asume que todas las profundidades $Z_i$ son similares entre si y pueden ser aproximadas por $Z_0$ que corresponde a la profundidad del punto de referencia $M_0$. La SOP de un punto $M_i$ es un punto $p_i$ en el plano $G$ con coordenadas ($x'_i$ , $y'_i $) con   
\begin{center}
$x'_i=f X_i/Z_0$  \qquad $y'_i=f Y_i/Z_0$ 
\end{center}
mientras que para para una proyección perspectiva lo que se obtiene es el punto $m_i$ de coordenadas  ($x_i$ , $y_i $) con
\begin{center}
$x_i=f X_i/Z_i$  \qquad $y_i=f Y_i/Z_i$ 
\end{center}


El factor $s=f/Z_0$ es el factor de escala que pone la S en SOP. Una vez que se realiza la proyección ortogonal del punto $M_i$ sobre el plano paralelo al plano imagen que contiene al punto $M_0$ que da lugar al punto $P_i$, se realiza la proyección perspectiva de este punto sobre el plano imagen. Analiticamente hacer esta proyeccion equivale a multiplicar las coordenadas del punto $P_i$ de coordenadas ($X_i$,$Y_i$,$Z_0$) por el factor de escala $s$. Las coordeanadas de la SOP tambien se pueden expresar como


$$x'_i=f X_0/Z_0+f(X_i-X_0)/Z_0=x_0 +s(X_i-X_0)$$
$$y'_i=y_0 +s(Y_i-Y_0)$$

En la figura \ref{fig:EstPose_Posit_2} se puede ver la construccion geometrica de la SOP.


%ESTAS COSAS (\input) A MI NO ME COMPILAN
% PABLO 

%\begin{figure}[h]
%\centering
% \input{../imagenes/EstPose/POSIT/EstPose_Posit_2.eps_tex}
%\caption{Proyeccion perspectiva y SOP.}
%\label{fig:EstPose_Posit_2}
%\end{figure}

                    
\subsection{Ecuaciones fundamentales}

Una vez obtenida la SOP de los puntos del modelo

Este algoritmo utiliza el algoritmo llamado POS(Pose from Orthography and Scaling), en el que se aproxima la pose por la pose obtenida a partir de una imagen SOP(Scale Orthographic Projection).
Una vez obtenida la pose aproximada se vuelve crear una imagen SOP y se estima una nueva pose. 
Este procedimiento es repetido hasta que la pose estimada sea buena.  
\section{validación de algoritmos}
\label{sec:Benchmark}

\begin{itemize}
\item ABABA
\item BABAB
\item ABABA
\item BABAB
\end{itemize}

