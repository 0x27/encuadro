\documentclass[journal]{IEEEtran}

%IDIOMA
\usepackage[spanish]{babel}
\usepackage[latin1]{inputenc}  % Ambos para solucin de asuntos de idioma
\usepackage[T1]{fontenc}

%MATH
\usepackage{amsmath,amssymb,mathrsfs,mathptmx}  % Matemticas varias
\usepackage{hyperref} % Para escribir URLs
\usepackage[]{algorithm2e}
\usepackage{verbatim}

%IMAGES
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{float}
\usepackage{subfigure}
\usepackage{wrapfig}
\usepackage[usenames,dvipsnames]{color}
\graphicspath{{../latex/}}
\DeclareGraphicsExtensions{.png,.jpg,.pdf,.mps,.gif,.bmp, .eps}
\usepackage{caption}

%VARIOS
\usepackage{multirow}
\usepackage{multicol}
\usepackage{tabulary}
\usepackage[table]{xcolor}
\usepackage{color}
\usepackage{listings}
\usepackage{tikz}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{2}

\begin{document}

\title{enCuadro\\
\huge Recorrido interactivo en museos con realidad aumentada sobre dispositivos móviles}

\author{Juan Braun, Martín Etchart, Pablo Flores y Mauricio González}% <-this % stops a space

% make the title area
\maketitle


\begin{abstract}
%\boldmath
La presente publicación es una breve síntesis de la investigación realizada y los resultados obtenidos durante el proyecto de fin de carrera llamado enCuadro. Aquí se resumen las distintas partes de la guía interactiva para museos que comprende: navegación dentro del museo, identificación de obras y realidad aumentada. También se explican algunos algoritmos elegidos que son base para la solución planteada que logró bueno resultados de realidad aumentada a través del procesamiento de imágenes en tiempo real. Finalmente se hace una breve demostración de algunos de los casos de uso más interesantes que se desarrollaron durante el proyecto.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
realidad aumentada, procesamiento, tiempo real, imagen, dispositivo.
\end{IEEEkeywords}

%\include{intro}
\section{Introducción}
% v5.0: corregida por el tribunal.
Debido a la creciente disponibilidad de las plataformas móviles y el gran poder de procesamiento con el que cuentan, el número de aplicaciones móviles ha crecido de manera significativa en los últimos a\~nos. Dichas plataformas cuentan con sistemas de adquisición de audio, video y una variedad de sensores como por ejemplo acelerómetro y giroscopio, lo que las transforma en sistemas ideales para desarrollar aplicaciones de procesamiento multimedia.

Por otro lado, desde hace algunos a\~nos varios museos de distintas partes del mundo han comenzado a considerar este tipo de dispositivos, y otras tantas tecnologías, como una alternativa muy interesante para brindar un valor agregado al usuario. Proyecciones de imágenes y videos, recorridos interactivos y aplicaciones de \textit{realidad aumentada} son tan s\'olo algunos de los ejemplos. Sin embargo, esta es un área muy reciente y en la que todavía queda un camino muy largo por recorrer.

Durante el proyecto de fin de carrera se desarrolló un recorrido interactivo para un museo con realidad aumentada, sobre dispositivos móviles con sistema operativo iOS como iPhone, iPod Touch y iPad. Probablemente, la realidad aumentada sea el mayor atractivo del proyecto por ser un área que se encuentra en pleno desarrollo y que todo el tiempo recibe ideas innovadoras y muy interesantes, lo que la hace por demás apasionante. Vale la pena entonces dar una definición para la misma:

\textit{La realidad aumentada (AR del inglés \textit{Augmented Reality}) es un término que denota la visión de un entorno físico del mundo real, cuyos elementos se combinan con elementos virtuales generados por computadora, para la creación de una realidad mixta en tiempo real.}\\

Cuando se genera una imagen por medio de realidad aumentada, conviven en ella elementos reales con elementos virtuales. Es básicamente un juego de percepciones. En la Figura \ref{fig: arIntro} se muestra un ejemplo de realidad aumentada desarrollado durante este proyecto. Se puede ver en ella, un cubo virtual sobre la esquina superior izquierda de ``Hombre de Vitruvio'' de Leonardo da Vinci, de manera coherente con la posición del dispositivo respecto de la obra. Si en esta figura tan sólo se viera a través del dispositivo, cualquiera podría pensar que el cubo es real y que efectivamente forma parte de la escena. Eso es lo que busca la realidad aumentada. 
\begin{figure}[H]
        \centering
        \includegraphics[scale=0.07]{figs_intro/arIntro.png}
         \caption{Ejemplo de realidad aumentada. En la figura se puede ver cómo se ubica un cubo virtual en la esquina superior izquierda de una réplica de la obra ``Hombre de Vitruvio'' de Leonardo da Vinci.}
         \label{fig: arIntro}
\end{figure}

%\include{alcance}
\section{Objetivos del proyecto}

El presente proyecto de fin de carrera tiene varios objetivos. En primer lugar se busca evaluar la capacidad de procesamiento de dispositivos móviles para aplicaciones de procesamiento de imágenes y estudiar el desempeño de diferentes algoritmos. Esto implica que se deben estudiar los diferentes dispositivos disponibles en el mercado. Por otro lado poder aplicar lo investigado para desarrollar una aplicación de realidad aumentada completa funcionando sobre un dispositivo móvil y en tiempo real. Finalmente se quiere utilizar dicha aplicación para abordar un problema real, el \textit{recorrido interactivo con realidad aumentada} para museos.

Los objetivos anteriores pueden resumirse en las tres tareas fundamentales del proyecto, que se expresan a continuación:

\begin{itemize}
\item[1.] \textbf{Investigación:} Comprensión de la arquitectura de las plataformas móviles y de sus plataformas de desarrollo, con el objetivo de implementar los distintos algoritmos y \textit{software} en general en las mismas. Estudio de las diferentes maneras de lograr la realidad aumentada, elección de los algoritmos a utilizar y su comprensión, desarrollo de nuevos algoritmos y variantes de algoritmos existentes. Aprendizaje de herramientas en general.
\item[2.] \textbf{Implementación}: Integración de los distintos bloques para lograr la realidad aumentada. Implementación de bloques lógicos accesorios que faciliten la integración de los primeros. Validación de los algoritmos utilizados y desarrollados.
\item[3.] \textbf{Aplicación}: Implementación de una aplicación completa en la que el usuario ingrese al museo, se ubique dentro de él, se dirija a un cuadro, reciba información respecto del mismo y finalmente experimente la realidad aumentada sobre la obra.
\end{itemize}

Cada una de ellas se jerarquizó en función de la importancia que se les dio en el proyecto, así como también el tiempo que se les dedicó:

$$
\begin{array}{|c|c|} \hline
\textbf{Frente de trabajo}			& \textbf{Porcentaje} \\ \hline
\small \textbf{Investigación}		&	50\% \\ \hline
\small \textbf{Implementación}	& 30\% \\ \hline
\small \textbf{Aplicación}			& 20\%  \\ \hline
\end{array}
$$

Lo que la tabla anterior intenta reflejar es que el foco principal del proyecto es la investigación, evaluación de algoritmos y su migración a plataformas móviles. La aplicación es un objetivo secundario que ayuda a validar los conceptos estudiados en las etapas de investigación e implementación. 

\begin{comment}
\section{Estado del arte}
Para llevar a cabo los objetivos planteados es bueno tener un contexto del estado del arte en cuanto al desarrollo de aplicaciones de realidad aumentada. Existen en la actualidad m\'ultiples kits de desarrollo comerciales para aplicaciones de realidad aumentada, en los que de manera sencilla, se logran este tipo de aplicaciones con desempe\~nos  muy buenos. Tal es el caso de \textit{Metaio} \cite{metaio12}, \textit{Vuforia} \cite{vuforia12}, \textit{String} \cite{string12} y \textit{Aurasma} \cite{aurasma12}. Por su parte, \textit{Layar} \cite{layar12}, es tambi\'en un kit de desarrollo para aplicaciones de realidad aumentada, pero se especializa en el agregado de contenido digital s\'olo sobre páginas impresas como revistas y catálogos. Ninguna de las herramientas anteriores es gratuita y tampoco \textit{open source}. En la Figura \ref{fig: metaioystring} se muestra un ejemplo que incluye por defecto \textit{Metaio} y otro que incluye \textit{String}.

\begin{figure}[H]
\centering
$$
\begin{array}{cc}
\includegraphics[scale=0.1665]{figs_alcance/metaio.png} & \includegraphics[scale=0.375]{figs_alcance/string.png}
\end{array}
$$
\caption{Izq.: Ejemplo de realidad aumentada que incluye por defecto el kit de desarrollo \textit{Metaio}. Der.: Otro ejemplo de realidad aumentada incluído por \textit{String}. En ambos casos debe notarse la coherencia lograda entre el mundo real y el mundo virtual. En la imagen de la derecha se tiene al personaje de \textit{String} parado en una mesa por delante de un mate y encima de un llavero.}
\label{fig: metaioystring}
\end{figure}

Si bien se probaron algunas de estas herramientas y se tomaron de ellas est\'andares en cuanto a la \textit{performance} que una aplicaci\'on de realidad aumentada debe tener, \textbf{no es un objetivo} del proyecto el desarrollo de aplicaciones de realidad aumentada con kits de desarrollo ya existentes. Se hace hincapi\'e en la investigaci\'on de herramientas y adquisici\'on de experiencia propia, con el objetivo de marcar una hoja de ruta para todo aquel que desee continuar la investigaci\'on con nuevas ideas y algoritmos. Todo el código generado en este proyecto, así como la documentación es \textit{open source} y se puede bajar del repositorio git ubicado en: \url{https://github.com/encuadro/encuadro}.
%Se deja a disposici\'on de cualquier interesado el c\'odigo de la aplicaci\'on final, los algoritmos implementados o editados, as\'i como toda la documentaci\'on.

Un último aspecto a destacar es que dentro del IIE hay poca experiencia en el área de realidad aumentada y en particular es la primera experiencia en plataformas móviles, por lo cual este proyecto tiene la importancia además de abrir el camino a nuevas generaciones que quieran abordar estos temas. 
\end{comment}

\section{Explicación global de la aplicación}
\label{sec: app}
Si bien se dijo que la creación de la aplicación integral, correspondiente a un recorrido con realidad aumentada para muesos, corresponde tan sólo a un quinto del alcance total del proyecto; la visualización de la aplicación total es quizá la forma más sencilla de comprender el proyecto en su conjunto y ayuda a evaluar si el sistema desarrollado funciona de manera aceptable.\\

La aplicación se desglosa en tres grandes bloques que son resumidos individualmente en secciones subsiguientes:
\begin{itemize}
\item \textbf{Navegación}
\item \textbf{Identificación de obras}
\item \textbf{Realidad aumentada}
\end{itemize}

\subsection{Navegación}
La navegación es la ubicación del usuario dentro del museo, la importancia de este bloque reside en que permite brindar información contextual de donde se encuentra el usuario. Esta información podría ser una breve descripción de la sala en la que se encuentra o indicaciones para ir de un lugar a otro del museo, entre otras. Desde el punto de vista de la aplicación también es importante porque permite manejar un menor volumen de datos al momento de identificar las obras, debido a que sabe en que sala se encuentra el usuario.  Se estudiaron distintas alternativas para la navegación. La primera posibilidad analizada fue la utilización de tres o más \textit{access points}, mediante los cuales, una vez mapeadas las características de las se\~nales en cada uno de los puntos de las salas, se puede ubicar al usuario dentro de las mismas. Otra forma de navegación que se tuvo en cuenta fue la localización a través de la tecnología GPS. Sin embargo, se optó por utilizar códigos QR dada su amplia difusión, practicidad y facilidad de implementación. La discusión técnica que justifica estas elecciones se encuentra detallada en la documentación del proyecto.\\

\subsection{Identificación de obras}
Por identificación de obras se entiende al proceso mediante el cual la aplicación detecta frente a qué obra se encuentra el usuario para así entonces brindarle información de la misma, una audioguía y si fuera el caso la posibilidad de desplegar realidad aumentada sobre ella. La forma en la que se implementó este bloque fue mediante un algoritmo de detección de características de imágenes llamado SIFT. Es un algoritmo que se basa en el contenido de la imagen únicamente, por lo que para identificar la obra no se necesita ningún tipo de marcador o agregado externo. Los detalles de la implementación se encuentran en la documentación del proyecto.\\

\subsection{Realidad Aumentada}

El proceso mediante el cual se logra la realidad aumentada puede verse en el diagrama de bloques de la Figura \ref{fig: bloques_ra}. 

\begin{figure}[h!]
\centering
\includegraphics[scale=0.9]{figs_alcance/bloques_ra.eps}
\caption{Diagrama de bloques del proceso mediante el cual se logra la realidad aumentada.}
\label{fig: bloques_ra}
\end{figure}

El primer bloque corresponde a la cámara que captura una imagen. En el segundo bloque dicha imagen es procesada con el objetivo de detectar en esta características. Estas características pueden ser segmentos, esquinas, descriptores. El tercer bloque corresponde a la estimación de pose, busca estimar en qué posición se encuentra la cámara respecto de cierto eje de coordenadas previamente definido y hacia dónde esta apunta.  En el cuarto bloque con la información anterior, se debe poder \textit{renderizar} una escena de manera consistente con la pose de la cámara, para así entonces lograr la salida del sistema, que será una imagen con la realidad aumentada incorporada.

Debe notarse que en ninguna etapa se mencionó ningún algoritmo en particular, ya sea para la detección de características como para la estimación de pose. Tampoco se menciona nada en particular para la etapa de \textit{rendering}. Esto se debe a que las etapas se concibieron de forma genérica, de manera ser libres de utilizar cualquier algoritmo de detección de características con cualquier algoritmo de estimación de pose. Las interfases entre los bloque están pensadas de forma de poder cambiar un bloque sin generar problemas en los demás bloques.  \\

%Para este proyecto se dise\~nó un marcador formado por tres grupos de cuadrados concéntricos, a partir del cual se extraen los segmentos necesarios para la estimación de la pose del dispositivo. El bloque de detección de características está compuesto por los bloques del diagrama de la Figura \ref{fig: bloques_correspondencias}.\\
%\begin{figure}[h!]
%\centering
%\includegraphics[scale=1.2]{figs_alcance/bloques_correspondencias.eps}
%\caption{Diagrama de bloques de la detección de características utilizada en este proyecto.}
%\label{fig: bloques_correspondencias}
%\end{figure}
%
%En el diagrama de bloques de la Figura \ref{fig: bloques_correspondencias}, primero se detectan idealmente todos los segmentos que hay en la imagen y luego se filtran tan sólo los pertenecientes al marcador antedicho y se agrupan por cuadrado. En el tercer bloque, se hallan las esquinas de estos cuadrados mediante la intersección de los segmentos. Finalmente estos puntos son ordenados de una manera predefinida. Detalles respecto de este proceso y sobre el marcador utilizado se ven en el Capítulo \ref{ch:marcadores}.\\

En lo que sigue se hace una explicación breve del modelo de cámara que fue necesario adoptar a los efectos de vincular las relaciones del mundo con las de las imágenes, se describe parcialmente la detección de características desarrollada y el algoritmo de estimación de pose que se utilizó.

\section{Modelo de cámara \textit{pin-hole}}
\label{sec:Calibracion de camara}

Este modelo consiste en un centro óptico O, en donde convergen todos los rayos de la proyección y un plano imagen en el cual la imagen es proyectada. Se define \textit{distancia focal} ($f$) como la distancia entre el centro óptico O y la intersección del eje óptico con el plano imagen (punto C). Ver Figura \ref{fig:CalibracionCamara}.  

\begin{figure}[h!]
\centering
\includegraphics[scale=0.4]{figs_camaraypose/CalibracionCamara.eps}
\caption{Modelo de cámara pin-hole.}
\label{fig:CalibracionCamara}
\end{figure}

Se llama proceso de proyección al proceso en el que se asocia al punto \textbf{M} del mundo, un punto \textbf{m} en la imagen. Para modelar el mismo es necesario referirse a varias transformaciones y varios ejes de coordenadas.
\begin{itemize}
\item \textit{Coordenadas del mundo}: son las coordenadas que describen la posición 3D del punto \textbf{M} respecto de los ejes del mundo $(u,v,w)$. La elección de los ejes del mundo es arbitraria.
\item \textit{Coordenadas de la cámara}: son las coordenadas que describen la posición del punto \textbf{M} respecto de los ejes de la cámara $(X,Y,Z)$. $i$, $j$ y $k$ son los versores de este eje de coordenadas.
\item \textit{Coordenadas de la imagen}: son las coordenadas que describen la posición del punto 2D, \textbf{m} respecto del centro del plano imagen, C. Los ejes de este sistema de coordenadas son $(x,y)$.
\item \textit{Coordenadas normalizadas de la imagen}: son las coordenadas que describen la posición del punto 2D, \textbf{m}, respecto del eje de coordenadas $(x',y')$ situado en la esquina superior izquierda del plano imagen.
\end{itemize}
La transformación que lleva al punto \textbf{M}, expresado respecto de los ejes del mundo, al punto \textbf{m}, expresado respecto del sistema de coordenadas normalizadas de la imagen, se puede ver como la composición de dos transformaciones menores. La primera, es la que realiza la proyección que transforma a un punto definido respecto del sistema de coordenadas de la cámara $(X, Y, Z)$ en otro punto sobre el plano imagen expresado respecto del sistema de coordenadas normalizadas de la imagen $(x',y')$. Véase que una vez calculada esta transformación, es una constante característica de cada cámara. Al conjunto de valores que definen esta transformación, se le llama ``{parámetros intrínsecos'' de la cámara. La segunda, es la transformación que lleva de expresar un punto respecto de los ejes del mundo $(u,v,w)$, a ser expresado según los ejes de la cámara $(X ,Y, Z)$. Esta última transformación varía conforme se mueve la cámara (respecto de los ejes del mundo) y el conjunto de valores que la definen es denominado ``parámetros extrínsecos'' de la cámara. Del cálculo de estos parámetros es que se obtiene la estimación de la pose de la cámara.
De lo anterior se concluye rápidamente que si se le llama $H$ a la matriz proyección total, tal que:
\[
m = H.M,
\]
entonces:
\[
H = I.E
\]
donde $I$ corresponde a la matriz proyección asociada a los parámetros intrínsecos y $E$ corresponde a la matriz asociada a los parámetros extrínsecos. 

\begin{itemize}
\item \textbf{Parámetros extrínsecos:} pose de la cámara.
\begin{itemize}
\item \underline{Traslación:} ubicación del centro óptico de la cámara respecto de los ejes del mundo.
\item \underline{Rotación:} rotación del sistema de coordenadas de la cámara $(X ,Y, Z)$, respecto de los ejes del mundo.
\end{itemize}
\item \textbf{Parámetros intrínsecos:} parámetros propios de la cámara. Dependen de su geometría interna y de su óptica.
\begin{itemize}
\item \underline{Punto principal (C = [$x'_C,y'_C$]):} es el punto intersección entre el eje óptico y el plano imagen. Las coordenadas de este punto vienen dadas en píxeles y son expresadas respecto del sistema normalizado de la imagen.
\item \underline{Factores de conversión píxel-milímetros ($d_x,d_y$):} indican el número de píxeles por milímetro que utiliza la cámara en las direcciones $x$ e $y$ respectivamente.
\item \underline{Distancia focal (f):} distancia entre el centro óptico (\textbf{O}) y el punto principal (\textbf{C}). Su unidad es el milímetro.
\item \underline{Factor de proporción (s):} indica la proporción entre las dimensiones horizontal y vertical de un píxel.   
\end{itemize}
\end{itemize}


\subsection{Matriz de proyección}
\label{sec:Matriz de proyección}

En la sección anterior se vio que es posible hallar una ``matriz de proyección'' H que dependa tanto de los parámetros intrínsecos de la cámara como de sus parámetros extrínsecos:
\[
m = H.M
\] 
donde \textbf{M} y \textbf{m} son los puntos ya definidos y vienen expresados en ``coordenadas homogéneas''. Por más información acerca de este tipo de coordenadas ver \cite{Hartley2004}.\\
Para determinar la forma de la matriz de proyección se estudia cómo se relacionan las coordenadas de \textbf{M} con las coordenadas de \textbf{m}; para hallar esta relación se debe analizar cada transformación, entre los sistemas de coordenadas mencionados con anterioridad, por separado.\\
\begin{itemize}
\item \textbf{Proyección 3D - 2D:} de las coordenadas homogéneas del punto \textbf{M} expresadas en el sistema de coordenadas de la cámara $(X_0,Y_0,Z_0,T_0)$, a las coordenadas homogéneas del punto \textbf{m} expresadas en el sistema de coordenadas de la imagen $(x_0,y_0,s_0)$:\\
Se desprende de la Figura \ref{fig:CalibracionCamara} y algo de trigonometría la siguiente relación entre las coordenadas en cuestión y la distancia focal (f):
\[
\frac{f}{Z_0} = \frac{x_0}{X_0} = \frac{y_0}{Y_0}
\]
A partir de la relación anterior:
\[
\left( \begin{array}{c}
x_0 \\
y_0
\end{array} \right)
=
\frac{f}{Z_0} 
\left( \begin{array}{c}
X_0 \\ 
Y_0
\end{array} \right)
\]
Expresado en forma matricial, en coordenadas homogéneas:
\[
\left( \begin{array}{c}
x_0 \\
y_0 \\
s_0
\end{array} \right)
= 
\left( \begin{array}{cccc}
f & 0 & 0 & 0 \\ 
0 & f & 0 & 0 \\
0 & 0 & 1 & 0
\end{array} \right)
\left( \begin{array}{c}
X_0 \\ 
Y_0 \\
Z_0 \\
1
\end{array} \right)
\]
\item \textbf{Transformación imagen - imagen:} de las coordenadas homogéneas del punto \textbf{m} expresadas respecto del sistema de coordenadas de la imagen $(x_0,y_0,s_0)$, a las coordenadas homogéneas de él mismo pero expresadas respecto del sistema de coordenadas normalizadas de la imagen $(x'_0,y'_0,s'_0)$:

Se les suma, a las coordenadas de \textbf{m} respecto del sistema de la imagen, la posición del punto C respecto del sistema normalizado de la imagen $(x'_C,y'_C)$. Las coordenadas de \textbf{m} dejan de ser expresadas en milímetros para ser expresadas en píxeles. Aparecen los factores de conversión $d_x$ y $d_y$:
\[
\begin{array}{c}
x'_0 = d_x.x_0 + x'_C \\
y'_0 = d_y.y_0 + y'_C
\end{array}
\]
Se obtiene entonces la siguiente relación matricial, en coordenadas homogéneas:
\[
\left( \begin{array}{c}
x'_0 \\
y'_0 \\
s'_0
\end{array} \right)
= 
\left( \begin{array}{ccc}
d_x & 0 & x'_C \\ 
0 & d_y & y'_C \\
0 & 0 & 1
\end{array} \right)
\left( \begin{array}{c}
x_0 \\ 
y_0 \\
1
\end{array} \right)
\]
\item \textbf{Matriz de parámetros intrínsecos $(I)$:} de las coordenadas homogéneas del punto \textbf{M} expresadas en el sistema de coordenadas de la cámara $(X_0,Y_0,Z_0,1)$, a las coordenadas homogéneas del punto \textbf{m} expresadas respecto del sistema de coordenadas normalizadas de la imagen $(x'_0,y'_0,s'_0)$:\\
Se obtiene combinando las dos últimas transformaciones. Nótese que como ya se aclaró, depende únicamente de parámetros propios de la construcción de la cámara:
\[
I = 
\left( \begin{array}{cccc}
d_x.f & 0 & x'_C & 0\\ 
0 & d_y.f & y'_C & 0\\
0 & 0 & 1 & 0
\end{array} \right)
\]

\item \textbf{Matriz de parámetros extrínsecos $(E)$:}  de las coordenadas homogéneas del punto \textbf{M} expresadas respecto del sistema de coordenadas del mundo $(U_0, V_0, W_0, P_0)$, a las coordenadas homogéneas de él mismo pero expresadas respecto del sistema de coordenadas de la cámara $(X_0,Y_0,Z_0,T_0)$:\\
Se obtiene de estimar la pose de la cámara respecto de los ejes del mundo y es la combinación de, primero una rotación $R_{3x3}$, y luego una traslación $T_{3x1}$. Se obtiene entonces la siguiente representación matricial:\\
\[
\left( \begin{array}{c}
X_0 \\
Y_0 \\
Z_0 \\
T_0
\end{array} \right)
= 
\left( \begin{array}{cc}
R & T \\ 
0 & 1
\end{array} \right)
\left( \begin{array}{c}
U_0 \\ 
V_0 \\
W_0 \\
P_0
\end{array} \right)
\]
donde la matriz de parámetros extrínsecos desarrollada toma la forma:
\[
E =
\left( \begin{array}{cccc}
i_u & i_v & i_w & t_{x} \\ 
j_u & j_v & j_w & t_{y}\\
k_u & k_v & k_w & t_{z} \\
0 & 0 & 0 & 1
\end{array} \right)
\]  
\item \textbf{Matriz de proyección $(H)$:}  de las coordenadas homogéneas del punto \textbf{M} expresadas respecto del sistema de coordenadas del mundo $(U_0, V_0, W_0, P_0)$, a las coordenadas homogéneas del punto \textbf{m} expresadas respecto del sistema de coordenadas normalizadas de la imagen $(x'_0,y'_0,s'_0)$:\\
Es la proyección total y se obtiene combinando las dos transformaciones anteriores:
\end{itemize}
\section{Marcadores}
La inclusión de \emph{marcadores} en la escena ayuda al problema de extracción de características y por lo tanto al problema de estimación de pose. Estos por construcción son elementos que presentan una detección estable en la imagen para el tipo de característica que se desea extraer así como medidas fácilmente utilizables para la estimación de la pose.

El marcador utilizado está basado en la estructura de detección incluida en los códigos \emph{QR} y se muestra en la Figura \ref{fig:Marker}. Éste consiste en tres grupos idénticos de tres cuadrados concéntricos superpuestos en ``capas''. La primer capa contiene el cuadrado negro de mayor tamaño, en la segunda capa se ubica el cuadrado mediano en color blanco y en la última capa un cuadrado negro pequeño. De esta forma se logra un fuerte contraste en los lados de cada uno de los cuadrados facilitando la detección de bordes o líneas. A diferencia de los códigos \emph{QR} la disposición espacial de los grupos de cuadrados es distinta para evitar ambigüedades en la identificación de los mismos entre sí. 
\begin{figure}[h!]
\centering
\includegraphics[scale=0.15]{figs_detection/Marker.eps}
\caption{Marcador propuesto basado en la estructura de detección de códigos QR.}
\label{fig:Marker}
\end{figure}

\subsection{Estructura del marcador}
\label{sec:detection_estructuras}
A continuación se presentan algunas definiciones de las estructuras básicas que componen el marcador propuesto. Estas son de utilidad para el diseño y forman un flujo natural y escalable para el desarrollo del algoritmo de determinación de correspondencias.

Los elementos más básicos en la estructura son los \emph{segmentos} los cuales consisten en un par de puntos en la imagen, $\mathbf{p} = (p_x,p_y)$ y $\mathbf{q} = (q_x,q_y)$. Estos \emph{segmentos} forman lo que son los lados del \emph{cuadrilátero}, el próximo elemento estructural del marcador.

Un \emph{cuadrilátero} o \emph{quadrilateral} en inglés, al que se le denomina $Ql$, está determinado por cuatro segmentos conexos y distintos entre sí. El cuadrilátero tiene dos propiedades notables; el \emph{centro} definido como el punto medio entre sus cuatro vértices y el \emph{perímetro} definido como la suma de el largo de sus cuatro lados.

A un \emph{conjunto de cuadriláteros} o \emph{quadrilateral set} se le denomina $QlSet$ y se construye a partir de $M$ cuadriláteros, con $M>1$. Los cuadriláteros comparten un mismo centro pero se diferencian en un factor de escala. A partir de dichos cuadriláteros se construye un lista ordenada $(Ql[0],Ql[1],\dots,Ql[M-1])$ en donde el orden viene dado por el valor de perímetro de cada $Ql$. Se define el \emph{centro del grupo de cuadriláteros}, $\mathbf{c}_i$, como el promedio de los centros de cada $Ql$ de la lista ordenada.

Finalmente el \emph{marcador QR} está constituido por $N$ conjuntos de cuadriláteros dispuestos en una geometría particular. Esta geometría permite la determinación de un sistema de coordenadas; un origen y dos ejes a utilizar. Se tiene una lista ordenada  $(QlSet[0],QlSet[1],\dots,QlSet[N-1])$ en donde el orden se puede determinar mediante la disposición espacial de los mismos o a partir de hipótesis razonables.

\subsection{Diseño}
En base a las estructuras previamente definidas es que se describe el diseño del marcador. Como ya se explicó se toma un marcador tipo QR basado en cuadriláteros y más específicamente en tres conjuntos de tres cuadrados dispuestos en como se muestra en la Figura \ref{fig:Marker}.

Los tres cuadriláteros correspondientes a un mismo conjunto de cuadriláteros tienen idéntica alineación e idéntico centro. Los diferencia un factor de escala, esto es, $Ql[0]$ tiene lado $l$ mientras que $Ql[1]$ y $Ql[2]$ tienen lado $2l$ y $3l$ respectivamente. Esto se puede ver en la Figura \ref{fig:QlSetDetail}. Adicionalmente se define un sistema de coordenadas con centro en el centro del $QlSet$ y ejes definidos como $\textbf{x}$ horizontal a la derecha e $\textbf{y}$ vertical hacia abajo.  Definido el sistema de coordenadas se puede fijar un orden a los vértices $v_{j_{1}}$ de cada cuadrilátero $Ql[j]$ como,
\begin{align*}
v_{j_{0}} = (a/2,a/2) && v_{j_{2}} = (-a/2,-a/2)  \\
v_{j_{1}} = (a/2,-a/2)&& v_{j_{3}} = (-a/2,a/2)
\end{align*}
con $a=(j+1)\times l$. El orden aquí explicado se puede ver también junto con el sistema de coordenadas en la Figura \ref{fig:MarkerDetail}.
\begin{figure}[h!]
\centering
\includegraphics[scale=0.15]{figs_detection/QlSetDetail.eps}
\caption{Detalle de un $QlSet$. A la izquierda se muestra el resultado de la detección de un $QlSet$ y el orden interno de sus cuadriláteros y a la derecha el orden de los vértices respecto al sistema de coordenadas local.}
\label{fig:QlSetDetail}
\end{figure}

Un detalle del marcador completo se muestra en la Figura \ref{fig:MarkerDetail} en donde se define el conjunto $i$ de cuadriláteros concéntricos como el $QlSet[i]$ y se definen los respectivos centros de cada uno de ellos como $\mathbf{c}_i$. El sistema de coordenadas del marcador QR tiene centro en el centro del $QlSet[0]$ y ejes de coordenadas idénticos al definido para cada $Ql$. Se tiene además que los ejes de coordenadas pueden ser obtenidos mediante los vectores normalizados,
\begin{equation}
\begin{split}
\mathbf{x}  = \frac{\mathbf{c}_1 - \mathbf{c}_0}{||\mathbf{c}_1-\mathbf{c}_0||} & \quad
\mathbf{y}  = \frac{\mathbf{c}_2 - \mathbf{c}_0}{||\mathbf{c}_2-\mathbf{c}_0||}
\end{split} 
\label{ec:detection_ejes}
\end{equation}

La disposición de los $QlSet$ es tal que la distancia indicada $d_{01}$ definida como la norma del vector entre los centros $\mathbf{c}_1$ y $\mathbf{c}_0$ es significativamente mayor que la distancia $d_{02}$ definida como la norma del vector entre los centros $\mathbf{c}_2$ y $\mathbf{c}_1$. Esto es, $d_{01}\gg d_{02}$. Este criterio facilita la identificación de los $QlSet$ entre sí basados únicamente en la posición de sus centros y es explicado en la sección de determinación de correspondencias.
\begin{figure}[h!]
\centering
\includegraphics[scale=0.13]{figs_detection/MarkerDetail.eps}
\caption{Detalle del marcador propuesto formando un sistema de coordenadas.}
\label{fig:MarkerDetail}
\end{figure}

\subsection{Diseño utilizado}
\textbf{Diseño de Test}: Durante el desarrollo e implementación de los algoritmos de detección e identificación de los vértices del marcador se trabajó con determinados parámetros de diseño de dimensiones apropiadas para posibilitar el traslado y las pruebas domésticas. 
\begin{itemize}
 \item $l = 30 mm$
 \item $d_{01} = 190 mm$
 \item $d_{02} = 100 mm$
\end{itemize}

\section{Detección}
La etapa de detección del marcador se puede separar en tres grandes bloques:
\begin{itemize}
 \item Detección de segmentos de línea.
 \item Filtrado y agrupamiento de segmentos.
 \item Determinación de correspondencias.
\end{itemize}
En esta sección se muestran algunos resultados para la detección de segmentos de línea por LSD y se centra en profundidad en los algoritmos desarrollados durante el proyecto para el filtrado de segmentos y determinación de correspondencias.

\subsection{Detección de segmentos de línea}
La detección de segmentos de línea se realiza mediante el uso del algoritmo LSD. En forma resumida, dicho algoritmo toma como entrada una imagen en escala de grises de tamaño $W\times H$ y devuelve una lista de segmentos en forma de pares de puntos de origen y destino. 

\subsection{Filtrado y agrupamiento de segmentos}
El filtrado y agrupamiento de segmentos consiste en la búsqueda de conjuntos de cuatro segmentos conexos en la lista de segmentos de línea detectados por LSD. Los conjuntos de segmentos conexos encontrados se devuelven en una lista en el mismo formato a la de LSD pero agrupados de a cuatro. A continuación se realiza una breve descripción del algoritmo de filtrado de segmentos implementado.

Se parte de una lista de $m$ segmentos de línea,
\begin{equation}
 \mathbf{L} = \begin{pmatrix}
               \mathbf{s}_0 & \mathbf{s}_1 & \dots & \mathbf{s}_{m-1} 
              \end{pmatrix}^t
\end{equation}
y se recorre en $i$ en busca de segmentos vecinos. La estrategia utilizada consiste en buscar, para el $i$-ésimo segmento $\mathbf{s}_i$, dos segmentos vecinos. En una primera etapa $\mathbf{s}_j$ y en una segunda etapa $\mathbf{s}_k$, de forma que se forme una ``U'' como se muestra en la Figura \ref{fig:SegmentosRectas}. La tercer etapa de búsqueda consiste en completar ese conjunto con un cuarto segmento $\mathbf{s}_l$ que cierre la ``U''.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.5]{figs_detection/SegmentosRectas.eps}
\caption{Conjunto de cuadriláteros conexos. A la izquierda la primera y segunda etapa del filtrado completadas para el segmento $\mathbf{s}_i$ en donde se busca una ``U''. A la derecha la última etapa en donde se cierra la ``U'' con el segmento $\mathbf{s}_l$.}
\label{fig:SegmentosRectas}
\end{figure}

Una vez encontrado el conjunto de cuatro segmentos conexos estos se marcan como utilizados, se guardan en una lista de salida y se continúa con el segmento $i+1$ hasta recorrer los $m$ segmentos de la lista de entrada. De esta forma se obtiene una lista de salida $\mathbf{S}$ de $n$ segmentos en donde $n$ es por construcción múltiplo de cuatro.

En la Figura \ref{fig:resultado_lsdfilt} se muestran lo resultados obtenidos para el algoritmo tomando como entrada la lista de segmentos de LSD. Se puede ver que los lados de los cuadrados del marcador son detectados correctamente pero también hay otras detecciones presentes. 
\begin{figure}[h!]
  \centering
  \subfigure[Entrada: segmentos de línea detectados por LSD]{
    \includegraphics[scale=0.25]{figs_detection/lsd.png}}
  \subfigure[Salida: segmentos de línea filtrados y agrupados]{
    \includegraphics[scale=0.25]{figs_detection/lsdfilt.png}}
  \caption{Resultados del algoritmo de filtrado y agrupamiento de segmentos de línea.}
  \label{fig:resultado_lsdfilt}
\end{figure}

El algoritmo descrito es simple y provee resultados aceptables en general pero es propenso a tanto a detectar \emph{falsos positivos} como al \emph{sobre-filtrado} algunos conjuntos. 

\subsection{Determinación de correspondencias}
\label{sec:detection_correspondencias}
Se detalla a continuación el algoritmo de determinación de correspondencias a partir de grupos de cuatro segmentos de línea conexos. 

Se toma como entrada la lista de segmentos filtrados y agrupados
\begin{equation}
\mathbf{S} = \begin{pmatrix}
			 \mathbf{s}_0 & \mathbf{s}_1 & \dots & \mathbf{s}_{i} & \mathbf{s}_{i+1} & \mathbf{s}_{i+2} & \mathbf{s}_{i+3} & \dots & \mathbf{s}_{n-1}
			 \end{pmatrix}^t
\end{equation}
en donde cada segmento se compone de un punto inicial $\mathbf{p}_{i}$ y un punto final $\mathbf{q}_{i}$, $\mathbf{s}_{i} = (\mathbf{p}_{i},\mathbf{q}_{i})$, con $n$ múltiplo de cuatro. Si $i$ también lo es, entonces el sub-conjunto, 
$\mathbf{S}_i = \begin{pmatrix}
				\mathbf{s}_{i} & \mathbf{s}_{i+1} & \mathbf{s}_{i+2} & \mathbf{s}_{i+3}
				\end{pmatrix}^t$, corresponde a un conjunto de cuatro segmentos del línea conexos.

Para cada sub-conjunto $\mathbf{S}_i$ se intersectan entre sí los segmentos obteniendo una lista de cuatro vértices,
$\mathbf{V}_i =  \begin{pmatrix}
		 \mathbf{v}_{i} & \mathbf{v}_{i+1} & \mathbf{v}_{i+2} & \mathbf{v}_{i+3} 
		 \end{pmatrix}^t$. 
Se obtienen dos posibles configuraciones que se muestran en la Figura \ref{fig:Vertices}, una de ellas tiene sentido horario y la otra antihorario partiendo de ${v}_i$.
\begin{figure}[h!]
\centering
\includegraphics[scale=0.5]{figs_detection/Vertices.eps}
\caption{Posibles configuraciones de vértices posterior a la intersección de conjuntos de segmentos pertenecientes a un cuadrilátero.}
\label{fig:Vertices}
\end{figure}

Posterior a la intersección se realiza un chequeo sobre el valor de las coordenadas de los vértices. Si alguno de ellos se encuentra fuera de los límites de la imagen, el conjunto de cuatro segmentos es marcado como inválido. Este chequeo resulta en el filtrado de ``falsos cuadriláteros'' corrigiendo un defecto del filtrado de segmentos, como por ejemplo un grupo de segmentos paralelos cercanos como ya se explicó. 

Para cada uno de los conjuntos de vértices se construye con ellos un elemento cuadrilátero que se almacena en una lista de cuadriláteros
\begin{eqnarray*}
 QlList =  \begin{pmatrix}
            Ql[0] & Ql[1] & \dots & Ql[i] & \dots & Ql[\frac{n}{4}]
           \end{pmatrix}^t
\end{eqnarray*}
% en sentido amplio, de dos segmentos contiguos, $s_i\cap s_{i+1}$ dadas las recta $r_1$ que pasa por los puntos $\mathbf{p}_1$, $\mathbf{q}_1$ del segmento $s_1$ y la recta $r_2$ que pasa por los puntos $\mathbf{p}_2$, $\mathbf{q}_2$ del segmento $s_2$, se determina el vértice correspondiente como la intersección $r_1 \cap r_2$. \\
% 
% y se intersectan en grupos de cuatro obteniendo cuatro vértices por cada grupo. Para cada grupo de vértices $v_k$ se construye un elemento cuadrilátero $\text{Ql}[k]$ que se almacena en una lista de cuadriláteros. 

A partir de esa lista de cuadriláteros, se buscan grupos de tres cuadriláteros $QlSet$ que ``compartan'' un mismo centro. Para esto se recorre ordenadamente la lista en $i$ buscando para cada cuadrilátero dos cuadriláteros $j$ y $k$ que cumplan que la distancia entre sus centros  y el del $i$-ésimo cuadrilátero sea menor a cierto umbral $c_{th}$,
\begin{equation}
\begin{split}
 d_{ij} = ||\mathbf{c}_i - \mathbf{c}_j||<c_{th}, & \quad  d_{ik} = ||\mathbf{c}_i - \mathbf{c}_k||<c_{th}.
\end{split}
\end{equation}
Estos cuadriláteros se marcan en la lista como utilizados con ellos se forma el $l$-ésimo $QlSet$ ordenándolos según su perímetro, de menor a mayor como  
$$QlSet[l] = \begin{pmatrix} Ql[0] & Ql[1] & Ql[2] \end{pmatrix}$$
con $l = (0,1,2)$. Esta búsqueda se realiza hasta encontrar un total de tres $QlSet$ completos de forma de obtener un marcador completo, esto es, detectando todos los cuadriláteros que lo componen. 

Una vez obtenida la lista de tres $QlSet$, 
$$QlSetList = \begin{pmatrix} QlSet[0] & QlSet[1] & QlSet[2] \end{pmatrix}$$
ésta se ordena de forma que su disposición espacial se corresponda con la del marcador QR. Para esto se calculan las distancias entre los centros de cada $QlSet$ y se toma el índice $i$ como el índice que produce el vector de menor distancia, $\mathbf{u}_i = \mathbf{c}_{i+1} -\mathbf{c}_i$. En este punto es importante que la condición de distancia entre los centros de los $QlSet$ se cumpla, $d_{10} \gg d_{20}$, para una simple identificación.

Una vez seleccionado el vector $\mathbf{u}_i$, se tienen obtiene el juego de vectores $(\mathbf{u}_i,\mathbf{u}_{i+1},\mathbf{u}_{i+2})$ como se muestra en la Figura \ref{fig:Centros}.
\begin{figure}[h!]
\centering
\includegraphics[scale=0.28]{figs_detection/Centros.eps}
\caption{Vértices de cada $Ql$ ordenados respecto al signo de sus proyecciones contra el sistema de coordenadas local a cada $QlSet$.}
\label{fig:Centros}
\end{figure}

Existen solo dos posibles configuraciones para estos vectores por lo que se utiliza este conocimiento para ordenar los $QlSet$ de la lista realizando el producto vectorial $\hat{\mathbf{u}_i} \times \hat{\mathbf{u}_{i+1}}$. 

Se construye un marcador QR que contiene la lista de tres $QlSet$ ordenados según lo indicado permitiendo la definición de un centro de coordenadas como el centro $\mathbf{c}_0$ del $QlSet[0]$ y ejes de coordenadas definidos en la Ecuación \ref{ec:detection_ejes}. 
De esta forma, recorriendo ordenadamente los elementos del marcador, se ordenan los vértices de cada $Ql$ del marcador.

Por último, a partir del marcador ordenado, se extrae una lista de vértices que se corresponde con la lista de vértices del marcador en coordenadas del mundo. Se determinan las correspondencias $\mathbf{M}_i\leftrightarrow \mathbf{m}_i$ necesarias para la estimación de pose. El algoritmo de determinación de correspondencias funciona correctamente por lo que los ``falsos'' cuadriláteros que sobreviven al filtrado de segmentos no son un problema.



\section{POSIT}
\label{POSIT}
A continuación se explica el algoritmo utilizado para el cálculo de la pose a partir de una imagen capturada por la cámara. Como lo dice el nombre del algoritmo se utiliza una técnica llamada \textit{POS} (\textit{P}ose from \textit{O}rtography and \textit{S}caling), esta técnica consiste en aproximar la pose de la cámara a partir de la proyección \textit{SOP}(\textit{S}caled \textit{O}rtographic \textit{Projection}). 

\subsection{Notación}
En la Figura \ref{fig: posit_1} se puede ver nuevamente el modelo de cámara pinhole. $O$ es el centro óptico y $G$ es el plano imagen ubicado  a una distancia focal $f$ de $O$. $x$ e $y$ son los ejes que apuntan en las direcciones de las filas y las columnas del sensor de la cámara respectivamente. $z$ es el eje que esta sobre el eje óptico de la cámara y apunta en sentido saliente. Los versores para estos ejes son \textbf{i}, \textbf{j} y \textbf{k} respectivamente.

Se considera ahora un objeto con puntos característicos $M_0$, $M_1$, ...., $M_i$, ...., $M_n$, cuyo eje de coordenadas está centrado en $M_0$ y está compuesto por los versores ($M_0u$, $M_0v$, $M_0w$ ). Como los ejes del mundo son arbitrarios se puede asumir sin pérdida de generalidad que los ejes del objeto coinciden con los ejes del mundo. La geometría del objeto se asume conocida, por ejemplo ($U_i$, $V_i$, $W_i$) son las coordenadas del punto $M_i$ en el marco de referencia del objeto.
Los puntos correspondientes a los puntos del objeto $M_i$ en la imagen son conocidos y se identifican como $m_i$, ($x_i$, $y_i$) son las coordenadas de este punto en la imagen\footnote{En realidad los puntos $m_i$ no están dados, vienen de la etapa anterior de detección.}. Las coordenadas de los puntos $M_i$ en el eje de coordenadas de la cámara, identificadas como ($X_i$, $Y_i$, $Z_i$), son desconocidas ya que no se conoce la pose del objeto respecto a la cámara.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.3]{figs_posit/posit_1.eps}
\caption{Proyección en perspectiva ($m_i$) y SOP ($p_i$) para un punto del modelo 3D $M_i$ y un punto de referencia del modelo $M_O$. Tomado de: \cite{DeMenthon95}.}
\label{fig: posit_1}
\end{figure}

Se busca computar la matriz de rotación y el vector de traslación del objeto respecto a la cámara. 


\subsection{SOP: \textit{S}caled \textit{O}rtographic \textit{P}rojection}
La proyección ortogonal escalada(SOP) es una aproximación a la proyección perspectiva. En esta aproximación se supone que las profundidades $Z_i$ de diferentes puntos $M_i$ en el eje de coordenadas de la cámara no difieren mucho entre sí, y por lo tanto se asume que todos los puntos $M_i$ tienen la misma profundidad que el punto $M_0$. Esta suposición es razonable cuando la relación distancia cámara objeto - profundidad del objeto es grande.

Para un punto $M_i$ la proyección perspectiva sobre el plano imagen está dada por:
\begin{align*}
x_i &= f X_i/Z_i,  & y_i& = fY_i/Z_i,
\end{align*}
mientras que la proyección SOP está dada por: 
\begin{align*}
x^{'}_i &= f X_i/Z_0, & y^{'}_i& = fY_i/Z_0. 
\end{align*}
Al término $s=f/Z_0$ se lo conoce como el factor de escala de la SOP.

En la Figura \ref{fig: posit_1} se puede ver como se construye la SOP. Primero se realiza la proyección ortogonal de todos los puntos $M_i$ sobre $K$, el plano paralelo al plano imagen que pasa por el punto $M_0$. Las proyecciones de los puntos $M_i$ sobre $K$ se llaman $P_i$. El segundo paso consiste en hacer la proyección perspectiva de los puntos $P_i$ sobre el plano imagen $G$ para obtener finalmente los puntos $p_i$. 

\subsection{Ecuaciones para calcular la proyección perspectiva}

La pose queda determinada si se conocen los vectores \textbf{i}, \textbf{j} y la coordenada \textbf{$Z_0$} del vector de traslación. La relación entre las coordenadas de los puntos $M_i$  en el sistema de coordenadas del objeto y el sistema de coordenadas de la cámara es
\begin{equation}\label{posit_eq1}
 \left(\begin{array}{ccc}
 X_i \\
 Y_i \\
 Z_i \\
 \end{array}\right) 
  = \left( \begin{array}{ccc}
i_u & i_v & i_w \\
j_u & j_v & j_w \\
k_u & k_v & k_w \end{array} \right) 
 \left(\begin{array}{ccc}
 U_i \\
 V_i \\
 W_i \\
 \end{array}\right) +
  \left(\begin{array}{ccc}
 X_0 \\
 Y_0\\
 Z_0 \\
 \end{array}\right) 
\end{equation}

La condición necesaria para que la pose definida por \textbf{i}, \textbf{j}, $x_0$, $y_0$ y $Z_0$ sea la pose exacta se puede expresar en las siguientes ecuaciones: 
\begin{equation}\label{posit_eq2}
\textbf{$M_0M_i$}\frac{f}{Z_0} \textbf{i}=x_i(1+\epsilon_i)-x_0
\end{equation}
\begin{equation}\label{posit_eq3}
\textbf{$M_0M_i$}\frac{f}{Z_0} \textbf{j}=y_i(1+\epsilon_i)-y_0
\end{equation}
donde $\epsilon_i$ se define como
\begin{equation}\label{posit_eq4}
\epsilon_i=\frac{1}{Z_0}\textbf{$M_0M_i$} \cdot \textbf{k}
\end{equation}
Se puede ver que los términos $x_i(1+\epsilon_i)$ y $y_i(1+\epsilon_i)$ son las coordenadas $(x^{'}_i,y^{'}_i)$ de la SOP, en el caso en que la pose está determinada. 

\subsection{Algoritmo}\label{sec:classicPosit4}
Las Ecuaciones \ref{posit_eq2} y \ref{posit_eq3} se puede reescribir como:
\begin{equation}\label{posit_eq8}
\textbf{$M_0M_i$} \textbf{I}=x_i(1+\epsilon_i)-x_0
\end{equation}
\begin{equation}\label{posit_eq9}
\textbf{$M_0M_i$} \textbf{J}=y_i(1+\epsilon_i)-y_0
\end{equation} 
en donde 
\begin{align}
\textbf{I}& = \frac{f}{Z_0}\textbf{i} = s\cdot \textbf{i},&  \textbf{J} = \frac{f}{Z_0}\textbf{j} = s\cdot\textbf{j}
\end{align}


Si se conocieran los valores exactos de los $\epsilon_i$ la pose obtenida de resolver el sistema de ecuaciones sería la pose exacta del objeto, como no se conocen los valores exactos de $\epsilon_i$ se utiliza un método iterativo que converge a la solución buscada. En la primera iteración se le toma $\epsilon_i = 0$. La ecuación para un punto cualquiera está dada por: 
\begin{equation}\label{posit_eq10}
\begin{split}
M_0M_i\cdot\textbf{I} = x^{'}_i - x_0\\
M_0M_j\cdot\textbf{J} = y^{'}_i - y_0
\end{split}
\end{equation}
Si se escribe la Ecuación \ref{posit_eq10} para los $n$ puntos del modelo, se tiene un sistema de $n$ ecuaciones con \textbf{I} y \textbf{J} como incógnitas
  \begin{equation}\label{posit_eq11}
 \begin{split}
 \textbf{A}\textbf{I} = x^{'} - x_0 \\
 \textbf{A}\textbf{J} = y^{'} - y_0
 \end{split}
 \end{equation}
\textbf{A} es una matriz $n\times3$ con las coordenadas de los puntos del modelo $M_i$ en el marco de coordenadas del objeto. Si se tienen más de 4 puntos y no son coplanares, la matriz \textbf{A} es de rango 3, y las soluciones al sistema están dadas por
 \begin{equation}\label{posit_eq12}
 \begin{split}
 \textbf{I} = \textbf{B}\left(x^{'} - x_0\right)\\
 \textbf{J} = \textbf{B}\left(y^{'} - y_0\right)
 \end{split}
 \end{equation}
donde \textbf{B} es la pseudo inversa de la matriz \textbf{A}. Se debe notar que la matriz \textbf{B} depende únicamente de la geometría del modelo que se asume conocida, por lo tanto solo es necesario calcularla una sola vez. 

Una vez obtenidos \textbf{I} y \textbf{J} se calculan \textit{s} y los versores \textbf{i}, \textbf{j} y \textbf{k}
\begin{subequations} \label{posit_eq13}
 \begin{align}
 s& =\left(\vert\textbf{I}\vert \vert\textbf{J}\vert\right)^{1/2} \\
 \textbf{i}& = \frac{\textbf{I}}{s} \\
 \textbf{j}& = \frac{\textbf{J}}{s} \\
 \textbf{k}& = \textbf{i} \times \textbf{j}
 \end{align}
 \end{subequations}
El vector traslación del centro del objeto al centro de la cámara es el vector $OM_0$ 
\begin{equation}\label{posit_eq14}
OM_0 = \frac{Z_0}{f}Om_0 = \frac{Om_0}{s}
\end{equation}
El vector $Om_0$ es conocido ya que se conocen las coordenadas de los puntos $m_i$, en particular $m_0$.

Una vez que se calcularon \textbf{i}, \textbf{j}, \textbf{k} y \textbf{T} se calculan los valores actualizados de $\epsilon_i$ según la Ecuación \ref{posit_eq4}. Si la variación de los $\epsilon_i$ es mayor a un determinado umbral, se repite el procedimiento actualizando las proyecciones SOP en la Ecuación \ref{posit_eq11}, si es menor al umbral se deja de iterar y se guarda la pose calculada.

\subsection{POSIT para puntos coplanares}\label{sec:classicPosit5}

Como se mencionó anteriormente, el algoritmo POSIT no funciona en el caso en que los puntos del modelo pertenecen a un mismo plano. Como los marcadores utilizados son planos, se buscó una versión de POSIT que resuelve el problema de la estimación de pose para este caso. El algoritmo fue escrito por DeMenthon et al. en \cite{CoplanarPosit}.


Si todos los puntos son coplanares, hay dos posibles configuraciones de puntos que cuya proyección SOP es la misma. Esto se puede ver en la Figura \ref{fig: posit_3}.
\begin{figure}[H]
\centering
\includegraphics[scale=0.25]{figs_posit/posit_3.eps}
\caption{Dos objetos dando la misma proyección SOP. Fuente: \cite{CoplanarPosit}.}
\label{fig: posit_3}
\end{figure}

Analíticamente sucede que el sistema de ecuaciones en \ref{posit_eq11} queda de rango 2. El vector solución que se obtiene al realizar la pseudo inversa de \textbf{A} es la proyección de \textbf{I} sobre el plano paralelo al plano imagen ($K$ en la Figura \ref{fig: posit_1}). Para determinar completamente el vector \textbf{I} hace falta calcular coordenada $z$ del vector \textbf{I}. Se procede de manera análoga para calcular el vector \textbf{J}. Finalmente se obtienen dos posibles soluciones. El cálculo detallado de los vectores \textit{I} y \textbf{J} para configuraciones coplanares se puede encontrar en \cite{CoplanarPosit}.   

En el caso en que las dos soluciones sean válidas para todas las iteraciones, el número de poses posibles sería $2^n$ a lo largo de $n$ iteraciones. En la práctica se manejan menos soluciones posibles. Se diferencian dos casos:
\begin{itemize}
\item[$\bullet$] Si se tiene que sólo una de las dos primeras poses calculadas es válida, en las siguientes iteraciones se da mismo comportamiento, por lo que hay solo un camino a seguir.

\item[$\bullet$] Si se tiene que las dos primeras poses calculadas son válidas, se abren dos posibles ramas. En la segunda iteración cada rama da lugar a dos nuevas poses, pero en este caso se toma la pose que da menor error de reproyección.

\end{itemize}

\section{Casos de Uso}
Se implementaron distintos casos de uso con el fin de integrar los algoritmos comentados en secciones anteriores en peque\~nas aplicaciones que funcionen \textit{de punta a punta}. Se buscó resolver individualmente los diferentes desafíos técnicos que una aplicación real de realidad aumentada para museos puede llegar a tener. Estas últimas no serán más que una combinación guionada de cada uno de estos casos de uso.

Se implementaron tres casos de uso: ``interactividad'', ``video'' y ``modelos''. El primero presenta un modelo simple sobre el marcador que responde a toques con cierto movimiento y un audio en particular, el segundo soluciona el problema de proyectar un video sobre el marcador de forma consistente con el movimiento del usuario. Para los casos de uso ``interactividad'' y ``modelos'' se utilizó la librería \textit{isgl3D} para poder realizar el \textit{rendering}. Esta librería fue ampliamente utilizada dada su fácil incorporación a las aplicaciones desarrolladas. La misma cuenta con modelos propios como el cubo que se ve en ``interactividad'' y tiene la posibilidad de importar otros modelos (generados con programas específicos para ello) de manera de lograr realidades aumentadas mucho más interesantes que si tan sólo se hicieran con modelos simples.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.15]{figs_casosuso/CasoUso1.PNG}
\caption{Captura de pantalla del caso de uso ``interactividad'. Se puede ver al cubo apoyado sobre el $QlSet$ de la esquina superior izquierda y los diferentes controles que ayudan a la depuración del código.}
\label{fig:CasoUso1}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.15]{figs_casosuso/CasoUso2}
\caption{Captura de pantalla del caso de uso ``video''. Se puede ver al video proyectado sobre el $QlSet$ de la esquina superior izquierda.}
\label{fig:CasoUso2}
\end{figure}

\begin{figure}[h!]
\centering
$$
\begin{array}{cc}
\includegraphics[scale=0.1]{figs_casosuso/CasoUso_3_1.png} & \includegraphics[scale=0.1]{figs_casosuso/CasoUso_3_2.png}
\end{array}
$$
\caption{Dos capturas de pantalla del caso de uso ``modelos'', desde dos ángulos disintos. Se pueden ver los modelos 3D pertenecientes a un perro chihuahue\~no y a dos sillones, uno de dos plazas y otro de tres.}
\label{fig:CasoUso3}
\end{figure}

\section{Aplicación}
A continuación se muestra la integración de los conocimientos adquiridos a lo largo del proyecto para poder llevar a cabo la realidad aumentada en una aplicación real. Si bien el objetivo principal del proyecto era la exploración de distintos métodos y algoritmos, parecía importante poder poner en práctica todo lo desarrollado en un producto final que pudiera parecerse a un prototipo de aplicación comercial. Se siguió la línea que se planteó desde un inicio que fue la de tener tres partes fundamentales: Navegación, Identificación y Realidad Aumentada. Para eso se incorporaron a la aplicación las siguientes funcionalidades:
 \begin{itemize}
\item Navegación por detección de QRs.
\item Identificación de obras mediante SIFT.
\item Comunicación con un servidor.
\item Navegación por listas de cuadros.
\item Interactividad con modelos.
\end{itemize}

\subsection{Diagrama global de la aplicación}
Para que sea más sencilla la comprensión de los bloques que componen la aplicación, en la Figura \ref{fig: Diaglobal} se muestra un diagrama esquemático de la misma que sirve para visualizar cómo es su flujo \textit{a nivel de usuario}.\\
\begin{figure}[h!]
\centering
\includegraphics[scale=0.8]{figs_implementacion/implementacion_bloques.eps}
\caption{Diagrama global de la aplicación.}
\label{fig: Diaglobal}
\end{figure}
Al comenzar el recorrido, el usuario tiene la opción de elegir cómo recorrer el museo: de manera \textit{autónoma} o de manera \textit{automática}. En la opción autónoma el usuario es el encargado de elegir dentro de una lista de autores el que más le interese, y dentro de la lista de cuadros del autor seleccionado, la obra que desea contemplar en detalle. De esta manera el usuario llega eligiendo opciones al cuadro de interés y está listo para comenzar la interacción con la obra, a través de audioguías o realidad aumentada. De la otra manera de recorrer el museo, con la opción automática, el usuario tiene la opción de navegar por el museo leyendo códigos QR desplegados en las distintas salas o secciones, que sirven para identificar en qué parte del museo se encuentra el usuario. De esta manera una vez que el usuario lee el QR, la aplicación lo reconoce y despliega una foto del autor y un mensaje que invita al usuario a continuar con el recorrido. 
\begin{figure}[h!]
\centering
$
\begin{array}{cc}
\includegraphics[scale=0.07]{figs_implementacion/artigas_1.png} & \includegraphics[scale=0.07]{figs_implementacion/artigas_2.png} \\
\includegraphics[scale=0.07]{figs_implementacion/artigas_3.png} & \includegraphics[scale=0.07]{figs_implementacion/artigas_4.png}
\end{array}
$
\caption{Escultura digital e interactiva de Artigas vista desde ángulos distintos y en posiciones distintas.}
\label{fig:artigases}
\end{figure}

Internamente la aplicación guarda la información en la que está el usuario y la utiliza en la siguiente etapa: identificación de la de obra. La identificación de la obra se da una vez que el usuario está frente a la misma y toma una foto de ella que es procesada y en pocos segundos la aplicación responde con la imagen original de la obra y el usuario puede comenzar la interacción con la obra, a través de audioguías o realidad aumentada. Ver Figura \ref{fig: Diaglobal}


\section{Conclusiones}
\label{sec: concluespe}
A continuación se detallan conclusiones separadas por los distintos bloques que componen la realidad aumentada.

\subsection{Detecci\'on de caracter\'isticas}
El bloque de detecci\'on de catacter\'isticas se expande en la Figura \ref{fig: bloques_correspondencias_conclu}. 

\begin{figure}[h!]
\centering
\includegraphics[scale=1.2]{figs_conclusiones/bloques_correspondencias.eps}
\caption{Diagrama de bloques de la detección de características utilizada en este proyecto.}
\label{fig: bloques_correspondencias_conclu}
\end{figure}

Se vio en secciones anteriores que la detección de segmentos se realizó utilizando LSD. Se está conforme tanto con el desempe\~no del algoritmo como con la optimización que se le hizo para tiempo real. Sin embargo, se cree que la implementación se puede optimizar aún más.\\

En lo que respecta a los algoritmos de filtrado y agrupación de segmentos, determinación de esquinas y correspondencias, se puede afirmar que funcionan correctamente. De cualquier manera, se cree que para ciertas aplicaciones en particular se podría utilizar un marcador más sencillo, con una geometría más simple de detectar; lo que simplificaría los algoritmos de detección y aumentaría la aplicabilidad a otros casos de uso.

\subsection{Estimaci\'on de pose}

En la sección \label{POSIT} se explicaron en detalle diferentes versiones de POSIT, el algoritmo que este proyecto utiliza para estimar la pose de la c\'amara a partir de ciertas caracter\'isticas de la imagen. En particular, se utiliz\'o \textbf{el POSIT coplanar moderno} que toma como entrada ciertos puntos en un modelo predefinido y sus correspondientes en la imagen. Este es un algoritmo muy preciso bajo ciertas condiciones y  veloz respecto de los algoritmos de estimación de pose tradicionales.  Sin embargo, resulta no del todo l\'ogico utilizar un algoritmo que tome puntos como entrada si las caracter\'isticas extra\'idas de la imagen primeramente son segmentos.

Existe por su parte, otra versión de POSIT denominada ``\textit{Soft} POSIT de líneas''. Esta versión, toma como entrada líneas en una imagen y líneas en un modelo y tiene la habilidad de detectar correspondencias entre ellas y luego estimar la pose de la cámara a partir de dichas correspondencias. De haberse usado esta versión de POSIT, el segundo y tercer bloque del diagrama de la Figura \ref{fig: bloques_correspondencias_conclu} no hubieran sido necesarios.

Sin embargo, \textit{Soft} POSIT de líneas no funciona para marcadores planos como los utilizados en este proyecto. Además, requiere de una estimación de la pose inicial del dispositivo lo que no hace tan trivial su uso. Por estos motivos se optó por utilizar POSIT coplanar y resolver las correspondencias en la etapa de filtrado de segmentos. 

\subsection{Una soluci\'on alternativa: SLAM}

``\textit{Simultaneous Localization And Mapping}'' (SLAM), o en espa\~nol ``Localización y Mapeado Simultáneos'', es el problema de ubicar a un dispositivo de visión artificial en un determinado lugar desconocido y que este, de manera incremental, vaya construyendo un modelo del lugar y a la vez vaya ubicándose dentro del modelo.

Este problema no sólo tiene solución, sino que además muchos de los kits de desarrollo de realidad aumentada lo utilizan. Y por lo que se pudo ver, da muy buenos resultados. En particular, logra resultados sustancialmente mejores a los de este proyecto, en cuanto a los tiempos de procesamiento.

De esta manera, los bloques ``Detección de características'' y ``Estimación de pose'' del diagrama de la Figura \ref{fig: bloques_ra}, pueden ser sustituidos por un algoritmo que solucione este problema.


\subsection{Dem\'as bloques}

El desempe\~no de los dem\'as bloques involucrados en el proceso mediante el cual se logra la realidad aumentada no requiere de un análisis extra al realizado en secciones anteriores. ISGL3D, la herramienta utilizada para realizar renders sobre las imágenes capturadas, funciona perfectamente y los dispositivos de captura y despliegue de imágenes del \textit{iPad} son muy buenos. Hubiera sido mejor capturar imágenes de mayor tama\~no, para así lograr una mejor calidad a la salida. Pero cuanto mayor es el tama\~no de la imagen capturada, tambi\'en lo es el tiempo de procesamiento de los algoritmos de detección, en particular LSD.


% use section* for acknowledgement
\section*{Agradecimientos}
\textit{
Los autores quieren agradecer especialmente a Rafael Grompone por sus consejos en la optimización de LSD y a Pablo Musé por su ayuda con el filtrado de Kalman. A Fernando Foglino por facilitar y permitir utilizar su modelo de Artigas.\\}

\textit{
A las comunidades de Software Libre y de Código Abierto, en particular a la comunidad de Acceso Abierto e Investigación Reproducible: IPOL.\\}

\textit{
Al Museo Nacional de Artes Visuales (MNAV) y al Museo Blanes. Finalmente también agradecer al Programa de Apoyo a la Investigación Estudiantil (PAIE) de la Comisión Sectorial de Investigación Científica (CSIC) por el apoyo económico para adquirir herramientas de desarrollo.\\}

%\bibliographystyle{IEEEtran}
%\bibliography{IEEEfull,articulo}

\bibliographystyle{unsrt}   
\bibliography{encuadro}  

% that's all folks
\end{document}


